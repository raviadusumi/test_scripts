{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAgSWEy7bV34EZ/eFqOphV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raviadusumi/test_scripts/blob/master/Test_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfZT5NQp8JMr",
        "colab_type": "code",
        "outputId": "349f8f5f-95e6-465b-b649-88d2a3ba06c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas_datareader import data as pdr \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set() #switches to seaborn defaults\n",
        "import fix_yahoo_finance as yf\n",
        "try:\n",
        "  # Use the %tensorflow_version magic if in colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcN3Q1P4sBPe",
        "colab_type": "code",
        "outputId": "eb9cc46f-ee96-47c0-cbd8-db3b93461c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone --branch r1.13.0 --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 2927, done.\u001b[K\n",
            "remote: Counting objects: 100% (2927/2927), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2447/2447), done.\u001b[K\n",
            "remote: Total 2927 (delta 512), reused 2068 (delta 405), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2927/2927), 369.04 MiB | 12.43 MiB/s, done.\n",
            "Resolving deltas: 100% (512/512), done.\n",
            "Checking out files: 100% (2768/2768), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oiYFPh4sejE",
        "colab_type": "code",
        "outputId": "ac31957a-8791-45bc-8a91-d02b9d3d66ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!tf_upgrade_v2 \\\n",
        "  --infile models/samples/cookbook/regression/custom_regression.py \\\n",
        "  --outfile /tmp/custom_regression_v2.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO line 38:8: Renamed 'tf.feature_column.input_layer' to 'tf.compat.v1.feature_column.input_layer'\n",
            "INFO line 43:10: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 46:17: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
            "INFO line 57:17: tf.losses.mean_squared_error requires manual check. tf.losses have been replaced with object oriented versions in TF 2.0 and after. The loss function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\n",
            "INFO line 57:17: Renamed 'tf.losses.mean_squared_error' to 'tf.compat.v1.losses.mean_squared_error'\n",
            "INFO line 61:15: Added keywords to args of function 'tf.shape'\n",
            "INFO line 62:15: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 65:40: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 68:39: Renamed 'tf.train.get_global_step' to 'tf.compat.v1.train.get_global_step'\n",
            "INFO line 83:9: tf.metrics.root_mean_squared_error requires manual check. tf.metrics have been replaced with object oriented versions in TF 2.0 and after. The metric function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\n",
            "INFO line 83:9: Renamed 'tf.metrics.root_mean_squared_error' to 'tf.compat.v1.metrics.root_mean_squared_error'\n",
            "INFO line 142:23: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 162:2: Renamed 'tf.logging.set_verbosity' to 'tf.compat.v1.logging.set_verbosity'\n",
            "INFO line 162:27: Renamed 'tf.logging.INFO' to 'tf.compat.v1.logging.INFO'\n",
            "INFO line 163:2: Renamed 'tf.app.run' to 'tf.compat.v1.app.run'\n",
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 1 files\n",
            "Detected 0 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'report.txt'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dymBavDcG1Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yfinance --upgrade --no-cache-dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK9XQPwLHu5N",
        "colab_type": "code",
        "outputId": "f37d3221-1d9e-438a-e906-c698dc869515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!tf_upgrade_v2 --infile !(Test.py) --outfile !(Test-up.py)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 0: `tf_upgrade_v2 --infile !(Test.py) --outfile !(Test-up.py)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGgjmG4-G3nS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yf.pdr_override()\n",
        "data = pdr.get_data_yahoo(\"NFLX\", start='2019-01-01').reset_index()\n",
        "data.to_csv('NFLX.csv', index=False)\n",
        "data.head() #show first five rows of data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w0STU2NG7Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('NFLX.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXhSrQr8HDBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "name = 'Actor-critic Duel agent'\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, name, input_size, output_size, size_layer):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "            feed_actor = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            tensor_action, tensor_validation = tf.split(feed_actor,2,1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            self.logits = feed_validation + tf.subtract(feed_action,\n",
        "                                                        tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, name, input_size, output_size, size_layer, learning_rate):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            self.REWARD = tf.placeholder(tf.float32, (None, 1))\n",
        "            feed_critic = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            tensor_action, tensor_validation = tf.split(feed_critic,2,1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            feed_critic = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "            feed_critic = tf.nn.relu(feed_critic) + self.Y\n",
        "            feed_critic = tf.layers.dense(feed_critic, size_layer//2, activation = tf.nn.relu)\n",
        "            self.logits = tf.layers.dense(feed_critic, 1)\n",
        "            self.cost = tf.reduce_mean(tf.square(self.REWARD - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "            \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 300\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "\n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, \n",
        "                                    self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n",
        "        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n",
        "        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
        "        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n",
        "        grads = zip(self.grad_actor, weights_actor)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "            \n",
        "    def _memorize(self, state, action, reward, new_state, dead):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X:[state]})[0]\n",
        "            action = np.argmax(prediction)\n",
        "        return action\n",
        "    \n",
        "    def _construct_memories_and_train(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states})\n",
        "        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states})\n",
        "        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X:states, self.critic.Y:Q})[0]\n",
        "        self.sess.run(self.optimizer, feed_dict={self.actor.X:states, self.actor_critic_grad:grads})\n",
        "        \n",
        "        rewards = np.array([a[2] for a in replay]).reshape((-1, 1))\n",
        "        rewards_target = self.sess.run(self.critic_target.logits, \n",
        "                                       feed_dict={self.critic_target.X:new_states,self.critic_target.Y:Q_target})\n",
        "        for i in range(len(replay)):\n",
        "            if not replay[0][-1]:\n",
        "                rewards[i] += self.GAMMA * rewards_target[i]\n",
        "        cost, _ = self.sess.run([self.critic.cost, self.critic.optimizer], \n",
        "                                feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.REWARD:rewards})\n",
        "        return cost\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self._select_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('actor-original', 'actor-target')\n",
        "                    self._assign('critic-original', 'critic-target')\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                state = next_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories_and_train(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XodwSa-oHJkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = data.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecOY_xP3HNsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exOT9gzrHQHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = buy_states)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = sell_states)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}